# -*- coding: utf-8 -*-
"""FNN

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-cHduNeCnXbw3Lt9U9eSX_WlBlGYbi5x

# Install Wandb
"""

!pip install wandb

"""# Importing the libraries"""

import numpy as np
import wandb
from keras.datasets import fashion_mnist, mnist
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix

"""# Data Pre-processing"""

(X,y),(X_test,y_test)=fashion_mnist.load_data()  ##Load data

X.shape ## X and X_test to be reshaped to (60000, 784(28x28)) array

num_features=784        ## 784 features
num_classes=np.max(y)+1 ## 10 classes

# Reshaping the training and test feature data 
X=np.reshape(X,(X.shape[0],784))
X_test=np.reshape(X_test,(X_test.shape[0],784))

# Normalize the pixel intensities
X=X/255
X_test=X_test/255

X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.1,random_state=123)  ## Splitting the training data to 90% training and 10% Validation data

### One hot encode the Class_labels (y_val & y_test & y_train)
def one_hot_encode(labels):
  z=np.zeros((10,len(labels)))
  for i in range(0,len(labels)):
    z[labels[i],i]=1  
  return z

y_val_encoded=one_hot_encode(y_val)
y_train_encoded=one_hot_encode(y_train)
y_test_encoded=one_hot_encode(y_test)

X=X.T
X_test=X_test.T
X_val=X_val.T
X_train=X_train.T

## Number of samples in training, validation and test set

no_sample_train=X_train.shape[1]
no_sample_val=X_val.shape[1]
no_sample_test=X_test.shape[1]

"""# Activation Functions & their derivatives"""

#### Activation functions and their derivatives

def sigmoid(a):
  return 1./(1.+np.exp(-a))

def sigmoid_derivative(a):
  return sigmoid(a)*(1-sigmoid(a))

def tanh(a):
  return np.tanh(a)

def tanh_derivative(a):
  return 1-np.power(tanh(a),2)

def relu(a):
  return np.maximum(0,a)

def relu_derivative(a):
  return 1*(a>0)

def identity(a):
  return a

def identity_derivative(a):
  return np.ones((np.shape(a)))

def softmax(a):
  #----
  ## a => np.array 
  #----
  return np.exp(a-np.max(a,axis=0))/(np.sum(np.exp(a-np.max(a,axis=0)),axis=0))  
  ## To prevent overflow error, the numpy array has been subtracted from the maximum value in that numpy array

def derivative_softmax(a):
  return softmax(a)*(1-softmax(a))

"""## Loss function

For l2 regularisation, the loss function becomes

$J=J+\frac{\lambda}{2m}\sum w^2$

m: batch-size
"""

## Loss function
def loss_computation(y_true,y_hat,loss,batch_size,lambda_val,parameters):

  ''' Function for loss computation
Parameters
----
y_true: True Class Label

y_hat: Predicted Class Label

loss: string
      cross_entropy/ mean-squared-error

batch-size: int

lambda_val: int
            lambda used in l2 regularisation

parameters: dict
            dictionary containing weight and bias parameters


Returns
----
J:  float 
    Loss value
'''

  
  if loss=='cross_entropy':
    J=(-1*np.sum(np.multiply(y_true,np.log(y_hat))))/batch_size
     
  elif loss=='mse':
    J=((1/2)*(np.sum((y_true-y_hat)**2)))/batch_size

  # L2 Regularisation
  sum_square_weight=0
  for i in range(1,(len(parameters)//2)+1):
    sum_square_weight+=np.sum(np.power(parameters['W_'+str(i)],2))
  
  J=J+(lambda_val/(2*batch_size))*sum_square_weight
  
  

  return J

class NeuralNetwork():
   def __init__(self,num_layers,activation_function,loss,batch_size,lambda_val):
      self.num_layers=num_layers
      self.num_hidden_layers=self.num_layers-2
      self.activation_function=activation_function
      self.loss=loss
      self.batch_size=batch_size
      self.lambda_val=lambda_val
      
   def weight_bias_initialize(self,neurons_per_layer,init):
        '''Initialise weights, biases, previous updates & look ahead parameters for different gradient descent algorithms

        Parameters
        ----
        neurons_per_layer: list
          list of number of neurons per layer in the structure [input_features,hiddenunits ,hiddenunits,..outputclasses]

        init: string
          initialisation type: default set to 'Xavier'

        Returns
        ----
        parameters: dictionary
          contains weights and biases. 

        old_parameters: dictionary
          previous updates initialisation. Used in nesterov, momemtum gradient descent

        look_ahead_parameters: dictionary
          copy of parameters, later used in nesterov gradient descent
        
        v: dictionary
          copy of old_parameters, later used in rmsprop, adam & nadam

        m: dictionary
          copy of old_parameters, later used in adam & nadam 

    '''

  # neurons_per_layer is a list specifying number of neurons per layer
        self.neurons_per_layer=neurons_per_layer
        self.init=init
        np.random.seed(42)

        self.parameters={}
        self.old_parameters={} ## For different kinds of gradient descent

        for i in range(1,len(self.neurons_per_layer)):
            if self.init=='Xavier':
                self.sdev=np.sqrt(2/(self.neurons_per_layer[i-1]+self.neurons_per_layer[i]))
                self.parameters['W_'+str(i)]=np.random.randn(self.neurons_per_layer[i],self.neurons_per_layer[i-1])*self.sdev
    
            if init=='random': # Random normal
                self.parameters['W_'+str(i)]=np.random.randn(self.neurons_per_layer[i],self.neurons_per_layer[i-1])*0.01

    
            self.parameters['b_'+str(i)]=np.zeros((self.neurons_per_layer[i],1))
  
            self.old_parameters['W_'+str(i)]=np.zeros((self.neurons_per_layer[i],self.neurons_per_layer[i-1]))
            self.old_parameters['b_'+str(i)]=np.zeros((self.neurons_per_layer[i],1))
  
        # For nesterov, adam, rmsprop, nadam
        self.look_ahead_parameters=self.parameters.copy()  
        self.v=self.old_parameters.copy()
        self.m=self.old_parameters.copy()           

        return self.parameters,self.old_parameters,self.look_ahead_parameters,self.v,self.m
    
   def forward_propagation(self,data,parameters):
    '''Function to forward propagate a minibatch of data once through the NN

    Parameters
    ----------
    data: np array

    parameter: dictionary
        Weight and biases

    activation_function: string
        activation function to be used except the output layer, default set to sigmoid

    Returns
    -------
    y_pred: np array
        contains the probability distribution for data sample after 1 pass
    activation: np array
        contains all post-activations values
    pre_activation: np array
        contains all pre-activations values

    '''
    self.parameters=parameters
    self.data=data
    self.activation = [0]*self.num_layers # activations
    self.pre_activation = [0]*self.num_layers # pre-activations
    
    self.activation[0] = self.data # H_1=training data
    
    for layer in range(1, self.num_layers):   # start from hidden layer 
        self.Weight = self.parameters["W_"+str(layer)]
        self.bias = self.parameters["b_"+str(layer)]
        
        self.pre_activation[layer] = np.matmul(self.Weight,self.activation[layer-1]) + self.bias    # a_i=W*h_(i-1) + b_i
        
        if layer == self.num_layers-1:
            self.activation[layer] = softmax(self.pre_activation[layer])
        else:
            if self.activation_function == 'sigmoid':
                self.activation[layer] = sigmoid(self.pre_activation[layer]) # h_i=g(a_i), g is the activation function
            elif self.activation_function == 'relu':
                self.activation[layer] = relu(self.pre_activation[layer])
            elif self.activation_function == 'tanh':
                self.activation[layer] = tanh(self.pre_activation[layer])
            elif self.activation_function== 'identity':
                self.activation[layer] = identity(self.pre_activation[layer])

            #####
            # Can add other activation functions here
            #####
                
    self.y_pred = self.activation[self.num_layers-1]  # output

    return self.y_pred,self.activation,self.pre_activation
    
   def backpropagate(self,y_hat,y_true,activation,pre_activation,parameters):  
    '''Function to calculate gradients

    Parameters
    ----------
    y_hat: np array
        output from forward propagation
    y_true: np array
        actual class labels
     
    activation: np array
        after-activations

    pre_activation: np array
        pre-activations   

    parameters: dict
        contains Weight and bias   

    activation_function: string
        activation function to be used except the output layer

    batch_size: int

    loss: string
        loss function: 'cross_entropy'/'mse'

    lamb: float
        L2 regularisation parameter: lambda

    Returns
    -------
    parameter_gradient: dict
        gradients wrt weight and biase

    '''

    self.layers_no_input=self.num_layers-1   ## No. of layers in NN exluding the input
    self.gradient_dA={}      ##Store Gradients wrt to pre-activations
    self.gradient_dH={}      ##Store Gradients wrt to after-activations
    self.parameter_gradient={}  ##Store gradients wrt to weight and bias
    self.y_true=y_true
    self.y_hat=y_hat
    self.pre_activation=pre_activation
    self.activation=activation
    self.parameters=parameters

  # Last_layer
    if self.loss=='cross_entropy':
        self.gradient_dA['dA_'+str(self.layers_no_input)]=-1*(self.y_true-self.y_hat)
  
    elif self.loss=='mse':
      self.gradient_dA['dA_'+str(self.layers_no_input)]=-1*(self.y_true-self.y_hat)*derivative_softmax(self.pre_activation[self.layers_no_input])
    
  
    for layer in range(self.layers_no_input,0,-1):  # move from Hidden layer L-1 to Hidden layer 1
        self.parameter_gradient['dW_'+str(layer)]=(np.dot(self.gradient_dA['dA_'+str(layer)],self.activation[layer-1].T)+self.lambda_val*self.parameters['W_'+str(layer)])/self.batch_size
        self.parameter_gradient['db_'+str(layer)]=np.sum(self.gradient_dA['dA_'+str(layer)],axis=1,keepdims=True)/self.batch_size  
    ### For batch_size I found this online
    ### Reference:https://datascience.stackexchange.com/questions/20139/gradients-for-bias-terms-in-backpropagation
    
        if layer>1:  
            if self.activation_function=='sigmoid':
                self.gradient_dH['dH_'+str(layer-1)]=np.matmul(self.parameters['W_'+str(layer)].T,self.gradient_dA['dA_'+str(layer)])
                self.gradient_dA['dA_'+str(layer-1)]=self.gradient_dH['dH_'+str(layer-1)]*sigmoid_derivative(self.pre_activation[layer-1])
    
            elif self.activation_function=='relu':
                self.gradient_dH['dH_'+str(layer-1)]=np.matmul(self.parameters['W_'+str(layer)].T,self.gradient_dA['dA_'+str(layer)])
                self.gradient_dA['dA_'+str(layer-1)]=self.gradient_dH['dH_'+str(layer-1)]*relu_derivative(self.pre_activation[layer-1])  

            elif self.activation_function=='tanh':
                self.gradient_dH['dH_'+str(layer-1)]=np.matmul(self.parameters['W_'+str(layer)].T,self.gradient_dA['dA_'+str(layer)])
                self.gradient_dA['dA_'+str(layer-1)]=self.gradient_dH['dH_'+str(layer-1)]*tanh_derivative(self.pre_activation[layer-1])   

            elif self.activation_function=='identity':
                self.gradient_dH['dH_'+str(layer-1)]=np.matmul(self.parameters['W_'+str(layer)].T,self.gradient_dA['dA_'+str(layer)])
                self.gradient_dA['dA_'+str(layer-1)]=self.gradient_dH['dH_'+str(layer-1)]*identity_derivative(self.pre_activation[layer-1])        

    return self.parameter_gradient


   def predict(self,data, parameters):
        self.data=data
        self.parameters=parameters
        
        self.output, _, _ = self.forward_propagation(self.data, self.parameters)
        self.predictions = np.argmax(self.output, axis=0)
        return self.predictions
   
   def loss_plot(self,train_loss,val_loss):
     self.train_loss=train_loss
     self.val_loss=val_loss
     plt.plot(list(range(0,len(self.train_loss))), self.train_loss, 'r', label="Training loss")
     plt.plot(list(range(0,len(self.val_loss))), self.val_loss, 'b', label="Validation loss")
     plt.title("Loss vs Epochs", size=10)
     plt.xlabel("Epochs", size=10)
     plt.ylabel("Loss", size=10)
     plt.legend()
     plt.show()


class NN_optimizers:
  def __init__(self,parameters,gradients,learning_rate,old_parameters,look_ahead_parameters,v,m,t,num_layers):
    self.parameters=parameters
    self.learning_rate=learning_rate
    self.old_parameters=old_parameters
    self.look_ahead_parameters=look_ahead_parameters
    self.gradients=gradients
    self.v=v
    self.m=m
    self.t=t
    self.num_layers=num_layers  

  
  def sgd(self):
    

    for i in range(1,self.num_layers): ## Since dictionary has keys 'W_1' to 'W_L'

      self.parameters['W_'+str(i)]=self.parameters['W_'+str(i)]-self.learning_rate*self.gradients['dW_'+str(i)]
      self.parameters['b_'+str(i)]=self.parameters['b_'+str(i)]-self.learning_rate*self.gradients['db_'+str(i)]
  
    return self.parameters
  
  def momentum_gd(self):
    self.beta=0.9

    for i in range(1,self.num_layers):
      self.old_parameters['W_'+str(i)]=self.beta*self.old_parameters['W_'+str(i)]+self.gradients['dW_'+str(i)]
      self.parameters['W_'+str(i)]-=self.learning_rate*self.old_parameters['W_'+str(i)]

      self.old_parameters['b_'+str(i)]=self.beta*self.old_parameters['b_'+str(i)]+self.gradients['db_'+str(i)]
      self.parameters['b_'+str(i)]-=self.learning_rate*self.old_parameters['b_'+str(i)]
  

    return self.parameters,self.old_parameters

  
   
  def nesterov_gd(self,train_data,train_label,activation_function,loss,batch_size,lambda_val):
    self.train_data=train_data
    self.train_label=train_label
    self.activation_function=activation_function
    self.loss=loss
    self.batch_size=batch_size
    self.lambda_val=lambda_val
    self.beta=0.9
    

    for i in range(1,self.num_layers):
      self.look_ahead_parameters['W_'+str(i)]=self.parameters['W_'+str(i)]-self.beta*self.old_parameters['W_'+str(i)]
      self.look_ahead_parameters['b_'+str(i)]=self.parameters['b_'+str(i)]-self.beta*self.old_parameters['b_'+str(i)]
        
    nn=NeuralNetwork(self.num_layers,activation_function,loss,batch_size,lambda_val)
    output,H,A=nn.forward_propagation(self.train_data,self.look_ahead_parameters)
    self.look_ahead_gradients=nn.backpropagate(output,self.train_label,H,A,self.look_ahead_parameters)

    for i in range(1,self.num_layers):
      self.old_parameters['W_'+str(i)]=self.beta*self.old_parameters['W_'+str(i)]+self.look_ahead_gradients['dW_'+str(i)]
      self.parameters['W_'+str(i)]-=self.learning_rate*self.old_parameters['W_'+str(i)]

      self.old_parameters['b_'+str(i)]=self.beta*self.old_parameters['b_'+str(i)]+self.look_ahead_gradients['db_'+str(i)]
      self.parameters['b_'+str(i)]-=self.learning_rate*self.old_parameters['b_'+str(i)]

    return self.parameters,self.old_parameters,self.look_ahead_parameters
   
    
  def rmsprop(self):
    self.beta=0.9
    self.epsilon=1e-7

    for i in range(1,self.num_layers):

      v_dw=self.beta*self.v['W_'+str(i)]+(1-self.beta)*np.power(self.gradients['dW_'+str(i)],2)
      v_db=self.beta*self.v['b_'+str(i)]+(1-self.beta)*np.power(self.gradients['db_'+str(i)],2)


      self.v['W_'+str(i)]=v_dw
      self.v['b_'+str(i)]=v_db

      self.parameters['W_'+str(i)]-=((self.learning_rate/np.sqrt(v_dw+self.epsilon))*self.gradients['dW_'+str(i)])
      self.parameters['b_'+str(i)]-=((self.learning_rate/np.sqrt(v_db+self.epsilon))*self.gradients['db_'+str(i)])
    return self.parameters, self.v
    
    
  def adam(self):

    self.beta1 = 0.9
    self.beta2 = 0.999
    self.epsilon = 1e-8

    for layers in range(1, self.num_layers):
        m_dw = self.beta1*self.m["W_"+str(layers)] + (1-self.beta1)*self.gradients["dW_"+str(layers)]
        v_dw = self.beta2*self.v["W_"+str(layers)] + (1-self.beta2)*np.power(self.gradients["dW_"+str(layers)],2)
        mw_hat = m_dw/(1.0 - self.beta1**self.t)
        vw_hat = v_dw/(1.0 - self.beta2**self.t)
        self.parameters["W_"+str(layers)] -= (self.learning_rate * mw_hat)/(np.sqrt(vw_hat + self.epsilon))

        m_db = self.beta1*self.m["b_"+str(layers)] + (1-self.beta1)*self.gradients["db_"+str(layers)]
        v_db = self.beta2*self.v["b_"+str(layers)] + (1-self.beta2)*np.power(self.gradients["db_"+str(layers)],2)
        mb_hat = m_db/(1.0 - self.beta1**self.t)
        vb_hat = v_db/(1.0 - self.beta2**self.t)

        self.parameters["b_"+str(layers)] -= (self.learning_rate * mb_hat)/(np.sqrt(vb_hat + self.epsilon))

        self.v["W_"+str(layers)] = v_dw
        self.m["W_"+str(layers)] = m_dw
        self.v["b_"+str(layers)] = v_db
        self.m["b_"+str(layers)] = m_db

    self.t = self.t + 1  # timestep
    return self.parameters, self.v, self.m, self.t

  
  def nadam(self,train_data,train_label,activation_function,loss,batch_size,lambda_val):
    self.train_data=train_data
    self.train_label=train_label
    self.activation_function=activation_function
    self.loss=loss
    self.batch_size=batch_size
    self.lambda_val=lambda_val
    self.beta=0.9
    self.beta1 = 0.9
    self.beta2 = 0.999
    self.epsilon = 1e-8


    for layers in range(1, self.num_layers):
      self.look_ahead_parameters["W_"+str(layers)] = self.parameters["W_"+str(layers)] - self.beta*self.old_parameters["W_"+str(layers)]
      self.look_ahead_parameters["b_"+str(layers)] = self.parameters["b_"+str(layers)] - self.beta*self.old_parameters["b_"+str(layers)]
    nn=NeuralNetwork(self.num_layers,activation_function,loss,batch_size,lambda_val)
    output,H,A=nn.forward_propagation(self.train_data,self.look_ahead_parameters)
    self.look_ahead_gradients=nn.backpropagate(output,self.train_label,H,A,self.look_ahead_parameters)
    
    for layers in range(1, self.num_layers):
      m_dw = self.beta1*self.m["W_"+str(layers)] + (1-self.beta1)*self.look_ahead_gradients["dW_"+str(layers)]
      v_dw = self.beta2*self.v["W_"+str(layers)] + (1-self.beta2)*np.power(self.look_ahead_gradients["dW_"+str(layers)],2)
      mw_hat = m_dw/(1.0 - self.beta1**self.t)
      vw_hat = v_dw/(1.0 - self.beta2**self.t)
      self.parameters["W_"+str(layers)] -= (self.learning_rate * mw_hat)/(np.sqrt(vw_hat) + self.epsilon)

      m_db = self.beta1*self.m["b_"+str(layers)] + (1-self.beta1)*self.look_ahead_gradients["db_"+str(layers)]
      v_db = self.beta2*self.v["b_"+str(layers)] + (1-self.beta2)*np.power(self.look_ahead_gradients["db_"+str(layers)],2)
      mb_hat = m_db/(1.0 - self.beta1**self.t)
      vb_hat = v_db/(1.0 - self.beta2**self.t)

      self.parameters["b_"+str(layers)] -= (self.learning_rate * mb_hat)/(np.sqrt(vb_hat) + self.epsilon)

      self.v["W_"+str(layers)] = v_dw
      self.m["W_"+str(layers)] = m_dw
      self.v["b_"+str(layers)] = v_db
      self.m["b_"+str(layers)] = m_db

    self.t = self.t + 1  # timestep


    return self.parameters, self.v, self.m, self.t



#### Fit Neural Network for wanb sweep

def NN_fit(train_data,train_labels,val_data,val_labels):
    
        train_data=train_data
        train_labels=train_labels
        val_data=val_data
        val_labels=val_labels
        num_neurons=64
        NN=NeuralNetwork(num_layers=5,activation_function='identity',loss="cross_entropy",batch_size=64,lambda_val=0)

        neurons_per_layer = [num_features] + [32]*NN.num_hidden_layers + [num_classes]
        parameters, old_parameters,look_ahead_parameters,v,m = NN.weight_bias_initialize(neurons_per_layer,init="Xavier") # initialize the parameters and past updates matrices
        
        optimizer='rmsprop'

  
        epoch_cost = []
        validation_epoch_cost = []
        epochs=40
        count = 1
        learning_rate=0.001
        t=1      

        while count<=epochs:
            count = count + 1
            if optimizer=='sgd':
               for i in range(0,train_data.shape[1]):
                 train_data_reshaped=train_data[:,i].reshape(num_features,1)
                 train_label_reshaped=train_labels[:,i].reshape(num_classes,1)
                 output,H,A=NN.forward_propagation(train_data_reshaped,parameters)
                 gradients=NN.backpropagate(output,train_label_reshaped,H,A,parameters)
                 optim=NN_optimizers(parameters,gradients,learning_rate,old_parameters,look_ahead_parameters,v,m,t,NN.num_layers)
                 parameters=optim.sgd()

            else:
              for i in range(0, train_data.shape[1], NN.batch_size):
            
                output,H,A = NN.forward_propagation(train_data[:,i:i+NN.batch_size],parameters)
                gradients = NN.backpropagate(output,train_labels[:,i:i+NN.batch_size],H,A,parameters)
                optim=NN_optimizers(parameters,gradients,learning_rate,old_parameters,look_ahead_parameters,v,m,t,NN.num_layers)

                if optimizer == 'nesterov':
                    parameters,old_parameters,look_ahead_parameters=optim.nesterov_gd(train_data[:,i:i+NN.batch_size],train_labels[:,i:i+NN.batch_size],NN.activation_function,NN.loss,NN.batch_size,NN.lambda_val)
                if optimizer=='adam':
                    parameters,v,m,t=optim.adam()
                if optimizer == 'rmsprop':
                    parameters,v =optim.rmsprop()
                elif optimizer == 'momentum':
                    parameters,old_parameters = optim.momentum_gd()
                elif optimizer == 'nadam':
                    parameters,v,m,t=optim.nadam(train_data[:,i:i+NN.batch_size],train_labels[:,i:i+NN.batch_size],NN.activation_function,NN.loss,NN.batch_size,NN.lambda_val)
              

        # loss for the full training set
            full_output, _, _ = NN.forward_propagation(train_data, parameters)
            cost = loss_computation(train_labels, full_output, NN.loss,no_sample_train, NN.lambda_val, parameters)
            epoch_cost.append(cost)
        
        # loss for the validation set
            out_val, _, _ = NN.forward_propagation(val_data, parameters)
            val_cost = loss_computation(val_labels, out_val,NN.loss,no_sample_val, NN.lambda_val, parameters)
            validation_epoch_cost.append(val_cost)
              
        # Training accuracy at the end of the epoch
            train_predictions = NN.predict(train_data, best_parameters)
            train_acc = accuracy_score(y_train, train_predictions)
           
        # Validation accuracy at the end of the epoch
            val_predictions = NN.predict(val_data, best_parameters)
            val_acc = accuracy_score(y_val, val_predictions)
            
        return epoch_cost,validation_epoch_cost,train_acc,val_acc
